{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSEo-elfY3ZC"
   },
   "outputs": [],
   "source": [
    "%%capture output\n",
    "!pip install lightning-uq-box\n",
    "!pip install SALib\n",
    "!pip install uncertainty-toolbox\n",
    "\n",
    "# Install library for learning deep UQ baselines.\n",
    "!git clone https://github.com/uncertainty-toolbox/simple-uq\n",
    "!pip install -e ./simple-uq\n",
    "%mv simple-uq/simple_uq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-TZcKM2KXn4y",
    "outputId": "28ceab97-af25-47fb-e3a2-c36066f8da35"
   },
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from lightning import LightningDataModule\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# from .utils import collate_fn_tensordataset\n",
    "\n",
    "import math\n",
    "\n",
    "def convert_float64(X):\n",
    "    return X.astype(np.float64)\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "from functools import partial\n",
    "\n",
    "import torch.nn as nn\n",
    "from laplace import Laplace\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "from lightning_uq_box.models import MLP\n",
    "from lightning_uq_box.uq_methods import MVERegression, DeterministicRegression, LaplaceRegression, NLL, BNN_VI_ELBO_Regression, BNN_VI_Regression\n",
    "from lightning_uq_box.viz_utils import (\n",
    "    plot_calibration_uq_toolbox,\n",
    "    plot_predictions_regression,\n",
    "    plot_toy_regression_data,\n",
    "    plot_training_metrics,\n",
    ")\n",
    "\n",
    "from SALib.sample import saltelli\n",
    "from SALib.analyze import sobol\n",
    "import seaborn as sns\n",
    "\n",
    "import uncertainty_toolbox as uct\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [14, 5]\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l0aMJkFjZqS2"
   },
   "outputs": [],
   "source": [
    "# Copyright (c) 2023 lightning-uq-box. All rights reserved.\n",
    "# Licensed under the Apache License 2.0.\n",
    "\n",
    "\"\"\"Utility functions for datamodules.\"\"\"\n",
    "\n",
    "def collate_fn_tensordataset(batch):\n",
    "    \"\"\"Collate function for tensor dataset to our framework.\"\"\"\n",
    "    inputs = torch.stack([item[0] for item in batch])\n",
    "    targets = torch.stack([item[1] for item in batch])\n",
    "    return {\"input\": inputs, \"target\": targets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SWFb158T0FUW",
    "outputId": "fe4f90c9-8980-4ef4-dc81-d8f2cd255ee5"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hr_BnILsUjfe"
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZkDiccT0IAp"
   },
   "outputs": [],
   "source": [
    "st_pete_property_df = pd.read_csv('/content/drive/MyDrive/why_people_stay/zillow_parcel_census_coast_flood_st_pete.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7PMlMzJQ2V9o",
    "outputId": "a113c0bf-c109-4ac2-bb5a-070bf5386212"
   },
   "outputs": [],
   "source": [
    "st_pete_property_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hy3lc5YU1z1m"
   },
   "outputs": [],
   "source": [
    "st_pete_property_df = st_pete_property_df[['zpid', 'streetAddress', 'zipcode', 'city', 'state',\n",
    "                                           'latitude', 'longitude', 'price', 'bathrooms', 'bedrooms', 'livingArea', 'POOL',\n",
    "                                           'EVAC_ZONE', 'GEOID', 'hhinc_k',\n",
    "                                           'flood_risk', 'MC_std', 'dist2coast']].dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w2Gd9FfQ2CBw",
    "outputId": "325ebcf8-c232-4abf-c644-60dbf8614912"
   },
   "outputs": [],
   "source": [
    "len(st_pete_property_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2SP6ua9kU3h"
   },
   "outputs": [],
   "source": [
    "st_pete_property_df['lg_price'] = np.log10(st_pete_property_df.price)\n",
    "st_pete_property_df['dist2coast_km'] = st_pete_property_df['dist2coast']/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "9CBICWtQtWl9",
    "outputId": "d2918274-a695-4e40-d383-b97b8cc57e38"
   },
   "outputs": [],
   "source": [
    "st_pete_property_df.EVAC_ZONE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzdeF_oFtpXp"
   },
   "outputs": [],
   "source": [
    "def merge_zone(value):\n",
    "    if value == 'A':\n",
    "        return 'A'\n",
    "    elif value == 'NON EVAC':\n",
    "        return 'NON EVAC'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "st_pete_property_df['EVAC_MERGE'] = st_pete_property_df.EVAC_ZONE.apply(merge_zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oti-gSRHvoM9"
   },
   "outputs": [],
   "source": [
    "other_df, test_df = train_test_split(st_pete_property_df, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(other_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZ3dF6YNEEzX"
   },
   "outputs": [],
   "source": [
    "cols = ['lg_price', 'bathrooms', 'livingArea', 'POOL', 'hhinc_k', 'flood_risk', 'dist2coast_km', 'latitude', 'longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xb1oImu52Qo7"
   },
   "outputs": [],
   "source": [
    "train_mod = train_df[cols]\n",
    "val_mod = val_df[cols]\n",
    "test_mod = test_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOVll1M-znaI"
   },
   "outputs": [],
   "source": [
    "numerical_columns = ['bathrooms', 'livingArea', 'hhinc_k', 'flood_risk', 'dist2coast_km', 'latitude', 'longitude']\n",
    "numerical_pipeline = make_pipeline(\n",
    "    FunctionTransformer(func=convert_float64, validate=False),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "categorical_columns = ['POOL']\n",
    "categorical_pipeline = make_pipeline(\n",
    "    OneHotEncoder(categories=\"auto\"),\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"numerical_preprocessing\", numerical_pipeline, numerical_columns),\n",
    "        (\"categorical_preprocessing\", categorical_pipeline, categorical_columns),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yi6X34hb9EN7",
    "outputId": "6875e5a3-5d23-405a-9f61-18d44518c9d3"
   },
   "outputs": [],
   "source": [
    "train_mod.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCoZum8rpwkg"
   },
   "outputs": [],
   "source": [
    "X_train = train_mod[train_mod.columns[1:]]\n",
    "Y_train = train_mod[train_mod.columns[0]]\n",
    "X_val = val_mod[val_mod.columns[1:]]\n",
    "Y_val = val_mod[val_mod.columns[0]]\n",
    "X_test = test_mod[test_mod.columns[1:]]\n",
    "Y_test = test_mod[test_mod.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j0suB_zXcVRa",
    "outputId": "18d75f30-c868-4c07-be19-c217d3a9295a"
   },
   "outputs": [],
   "source": [
    "# Fit the preprocessor first\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Extract feature names for numerical columns\n",
    "numerical_feature_names = numerical_columns\n",
    "\n",
    "# Extract feature names for categorical columns using the OneHotEncoder step\n",
    "categorical_feature_names = (\n",
    "    preprocessor.named_transformers_['categorical_preprocessing']\n",
    "    .named_steps['onehotencoder']\n",
    "    .get_feature_names_out(categorical_columns)\n",
    ")\n",
    "\n",
    "# Combine both numerical and categorical feature names\n",
    "all_feature_names = np.concatenate([numerical_feature_names, categorical_feature_names])\n",
    "\n",
    "# Display the feature names\n",
    "print(\"Feature Names After Transformation:\")\n",
    "print(all_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoAAW1tlZt_l"
   },
   "outputs": [],
   "source": [
    "# def generate_multivariate_y(x):\n",
    "#     \"\"\"Custom function to generate dependent variable with noise.\"\"\"\n",
    "#     noise = np.random.normal(scale=0.1, size=(x.shape[0],))  # Add some noise\n",
    "#     y = 3 * x[:, 0] + 2 * x[:, 1] - x[:, 2] + np.sin(x[:, 3]) - 0.5 * x[:, 4] ** 2 + 0.1 * x[:, 5] + noise\n",
    "#     return y\n",
    "\n",
    "\n",
    "# class CustomMultivariateDatamodule(LightningDataModule):\n",
    "#     \"\"\"Implement Dataset with 7 independent variables and 1 dependent variable.\"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         n_points: int = 500,\n",
    "#         batch_size: int = 100,\n",
    "#         test_fraction: float = 0.1,\n",
    "#         val_fraction: float = 0.1,\n",
    "#         calib_fraction: float = 0.4,\n",
    "#         noise_seed: int = 42,\n",
    "#         split_seed: int = 42,\n",
    "#     ) -> None:\n",
    "#         \"\"\"Define a multivariate regression dataset.\n",
    "\n",
    "#         Split `n_points` data points into train, validation, and test sets.\n",
    "\n",
    "#         Args:\n",
    "#             n_points: Number of data points to generate.\n",
    "#             batch_size: Batch size for data loader.\n",
    "#             test_fraction: Fraction of n_points for test set.\n",
    "#             val_fraction: Fraction of n_points for validation set.\n",
    "#             calib_fraction: Fraction of n_points for calibration set.\n",
    "#             noise_seed: Random seed for data generation.\n",
    "#             split_seed: Random seed for train/test/val split.\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "\n",
    "#         np.random.seed(noise_seed)\n",
    "#         self.batch_size = batch_size\n",
    "\n",
    "#         # Generate independent variables (X) and dependent variable (Y)\n",
    "#         x = np.random.uniform(-5, 5, size=(n_points, 7))\n",
    "#         y = generate_multivariate_y(x)\n",
    "\n",
    "#         # full dataset\n",
    "#         self.X_all = x\n",
    "#         self.Y_all = y[:, None]  # Make Y a 2D array\n",
    "\n",
    "#         # Split data into train and held-out IID test\n",
    "#         X_other, self.X_test, Y_other, self.Y_test = train_test_split(\n",
    "#             self.X_all, self.Y_all, test_size=test_fraction, random_state=split_seed\n",
    "#         )\n",
    "\n",
    "#         # Split train data into train and validation\n",
    "#         self.X_train, self.X_val, self.Y_train, self.Y_val = train_test_split(\n",
    "#             X_other,\n",
    "#             Y_other,\n",
    "#             test_size=val_fraction / (1 - test_fraction),\n",
    "#             random_state=split_seed,\n",
    "#         )\n",
    "\n",
    "#         # Split validation data into validation and calibration (for conformal)\n",
    "#         self.X_val, self.X_calib, self.Y_val, self.Y_calib = train_test_split(\n",
    "#             self.X_val, self.Y_val, test_size=calib_fraction, random_state=split_seed\n",
    "#         )\n",
    "\n",
    "#         # Fit scalers on train data\n",
    "#         scalers = dict(\n",
    "#             X=StandardScaler().fit(self.X_train), Y=StandardScaler().fit(self.Y_train)\n",
    "#         )\n",
    "\n",
    "#         # Apply scaling to all splits, convert to torch tensors\n",
    "#         for xy in [\"X\", \"Y\"]:\n",
    "#             for arr_type in [\"train\", \"test\", \"val\", \"calib\", \"all\"]:\n",
    "#                 arr_name = f\"{xy}_{arr_type}\"\n",
    "#                 setattr(\n",
    "#                     self,\n",
    "#                     arr_name,\n",
    "#                     self._n2t(scalers[xy].transform(getattr(self, arr_name))),\n",
    "#                 )\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _n2t(x):\n",
    "#         return torch.from_numpy(x).type(torch.float32)\n",
    "\n",
    "#     def train_dataloader(self) -> DataLoader:\n",
    "#         \"\"\"Return train dataloader.\"\"\"\n",
    "#         return DataLoader(\n",
    "#             TensorDataset(self.X_train, self.Y_train),\n",
    "#             batch_size=self.batch_size,\n",
    "#             shuffle=True,\n",
    "#             collate_fn=collate_fn_tensordataset,\n",
    "#         )\n",
    "\n",
    "#     def val_dataloader(self) -> DataLoader:\n",
    "#         \"\"\"Return val dataloader.\"\"\"\n",
    "#         return DataLoader(\n",
    "#             TensorDataset(self.X_val, self.Y_val),\n",
    "#             batch_size=self.batch_size,\n",
    "#             collate_fn=collate_fn_tensordataset,\n",
    "#         )\n",
    "\n",
    "#     def calib_dataloader(self) -> DataLoader:\n",
    "#         \"\"\"Return calibration dataloader.\"\"\"\n",
    "#         return DataLoader(\n",
    "#             TensorDataset(self.X_calib, self.Y_calib),\n",
    "#             batch_size=self.batch_size,\n",
    "#             collate_fn=collate_fn_tensordataset,\n",
    "#         )\n",
    "\n",
    "#     def test_dataloader(self) -> DataLoader:\n",
    "#         \"\"\"Return test dataloader.\"\"\"\n",
    "#         return DataLoader(\n",
    "#             TensorDataset(self.X_test, self.Y_test),\n",
    "#             batch_size=self.batch_size,\n",
    "#             collate_fn=collate_fn_tensordataset,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-ET-7mNteim"
   },
   "outputs": [],
   "source": [
    "class CustomMultivariateDatamodule(LightningDataModule):\n",
    "    \"\"\"Implement Dataset with 7 independent variables and 1 dependent variable.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # n_points: int = 500,\n",
    "        batch_size: int = 100,\n",
    "        test_fraction: float = 0.1,\n",
    "        val_fraction: float = 0.1,\n",
    "        calib_fraction: float = 0.4,\n",
    "        # noise_seed: int = 42,\n",
    "        split_seed: int = 42,\n",
    "    ) -> None:\n",
    "        \"\"\"Define a multivariate regression dataset.\n",
    "\n",
    "        Split `n_points` data points into train, validation, and test sets.\n",
    "\n",
    "        Args:\n",
    "            n_points: Number of data points to generate.\n",
    "            batch_size: Batch size for data loader.\n",
    "            test_fraction: Fraction of n_points for test set.\n",
    "            val_fraction: Fraction of n_points for validation set.\n",
    "            calib_fraction: Fraction of n_points for calibration set.\n",
    "            noise_seed: Random seed for data generation.\n",
    "            split_seed: Random seed for train/test/val split.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # np.random.seed(noise_seed)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # # full dataset\n",
    "        # self.X_all = X\n",
    "        # self.Y_all = np.array(Y)[:, None]  # Make Y a 2D array\n",
    "\n",
    "        # # Split data into train and held-out IID test\n",
    "        # X_other, self.X_test, Y_other, self.Y_test = train_test_split(\n",
    "        #     self.X_all, self.Y_all, test_size=test_fraction, random_state=split_seed\n",
    "        # )\n",
    "\n",
    "        # # Split train data into train and validation\n",
    "        # self.X_train, self.X_val, self.Y_train, self.Y_val = train_test_split(\n",
    "        #     X_other,\n",
    "        #     Y_other,\n",
    "        #     test_size=val_fraction / (1 - test_fraction),\n",
    "        #     random_state=split_seed,\n",
    "        # )\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = np.array(Y_train)[:, None]\n",
    "        self.X_val = X_val\n",
    "        self.Y_val = np.array(Y_val)[:, None]\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = np.array(Y_test)[:, None]\n",
    "\n",
    "        # Split validation data into validation and calibration (for conformal)\n",
    "        self.X_val, self.X_calib, self.Y_val, self.Y_calib = train_test_split(\n",
    "            self.X_val, self.Y_val, test_size=calib_fraction, random_state=split_seed\n",
    "        )\n",
    "\n",
    "        # Fit scalers on train data\n",
    "        scalers = dict(\n",
    "            X=preprocessor.fit(self.X_train), Y=StandardScaler().fit(self.Y_train)\n",
    "        )\n",
    "\n",
    "        # Apply scaling to all splits, convert to torch tensors\n",
    "        for xy in [\"X\", \"Y\"]:\n",
    "            for arr_type in [\"train\", \"test\", \"val\", \"calib\"]:\n",
    "                arr_name = f\"{xy}_{arr_type}\"\n",
    "                processed_arr = scalers[xy].transform(getattr(self, arr_name))\n",
    "                if xy == \"X\":\n",
    "                  processed_arr = np.delete(processed_arr, -2, axis=1)\n",
    "                setattr(\n",
    "                    self,\n",
    "                    arr_name,\n",
    "                    self._n2t(processed_arr),\n",
    "                )\n",
    "\n",
    "    @staticmethod\n",
    "    def _n2t(x):\n",
    "        return torch.from_numpy(x).type(torch.float32)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Return train dataloader.\"\"\"\n",
    "        assert isinstance(self.X_train, torch.Tensor), \"X_train is not a Tensor\"\n",
    "        assert isinstance(self.Y_train, torch.Tensor), \"Y_train is not a Tensor\"\n",
    "        assert self.X_train.size(0) == self.Y_train.size(0), \"Size mismatch in train data\"\n",
    "\n",
    "        return DataLoader(\n",
    "            TensorDataset(self.X_train, self.Y_train),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn_tensordataset,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Return val dataloader.\"\"\"\n",
    "        return DataLoader(\n",
    "            TensorDataset(self.X_val, self.Y_val),\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=collate_fn_tensordataset,\n",
    "        )\n",
    "\n",
    "    def calib_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Return calibration dataloader.\"\"\"\n",
    "        return DataLoader(\n",
    "            TensorDataset(self.X_calib, self.Y_calib),\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=collate_fn_tensordataset,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Return test dataloader.\"\"\"\n",
    "        return DataLoader(\n",
    "            TensorDataset(self.X_test, self.Y_test),\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=collate_fn_tensordataset,\n",
    "        )\n",
    "\n",
    "    # def gt_dataloader(self) -> DataLoader:\n",
    "    #     \"\"\"Return test dataloader.\"\"\"\n",
    "    #     return DataLoader(\n",
    "    #         TensorDataset(self.X_gtext, self.Y_gtext),\n",
    "    #         batch_size=self.batch_size,\n",
    "    #         collate_fn=collate_fn_tensordataset,\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5GaS53KGaKAT"
   },
   "outputs": [],
   "source": [
    "dm = CustomMultivariateDatamodule()\n",
    "\n",
    "X_train, Y_train, train_loader, X_test, Y_test, test_loader = (\n",
    "    dm.X_train,\n",
    "    dm.Y_train,\n",
    "    dm.train_dataloader(),\n",
    "    dm.X_test,\n",
    "    dm.Y_test,\n",
    "    dm.test_dataloader(),\n",
    "    # dm.X_gtext,\n",
    "    # dm.Y_gtext,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WtlPbXInklxP",
    "outputId": "03cf844d-4b94-4aed-d596-c6c82a0306da"
   },
   "outputs": [],
   "source": [
    "\"\"\"Perform Variance-Based Sensitivity Analysis using Sobol sampling and analysis.\"\"\"\n",
    "problem = {\n",
    "    'num_vars': X_train.shape[1],\n",
    "    'names': all_feature_names.tolist(),\n",
    "    'bounds': [[X_train[:,i].numpy().min(), X_train[:,i].numpy().max()] for i in range(5)]\n",
    "}\n",
    "\n",
    "param_values = saltelli.sample(problem, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ji85zHJeEz8",
    "outputId": "97ac6066-18c0-4924-e7ea-febc426d4fb7"
   },
   "outputs": [],
   "source": [
    "def enforce_one_hot_last_n_columns(samples, n):\n",
    "    \"\"\"\n",
    "    Enforce valid one-hot encoding only for the last `n` columns.\n",
    "\n",
    "    Args:\n",
    "        samples (numpy.ndarray): The input array.\n",
    "        n (int): The number of columns to apply one-hot encoding to (from the end).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The modified array with valid one-hot encoding enforced on the last `n` columns.\n",
    "    \"\"\"\n",
    "    valid_samples = []\n",
    "    for row in samples:\n",
    "        # Split the row into non-categorical and categorical parts\n",
    "        non_categorical = row[:-n]\n",
    "        categorical = row[-n:]\n",
    "\n",
    "        # Enforce one-hot encoding only on the categorical part\n",
    "        one_hot = np.zeros(n)\n",
    "        one_hot[np.argmax(categorical)] = 1  # Set only the highest value to 1\n",
    "\n",
    "        # Reconstruct the row with non-categorical + modified categorical\n",
    "        valid_samples.append(np.concatenate([non_categorical, one_hot]))\n",
    "\n",
    "    return np.array(valid_samples)\n",
    "\n",
    "# Example usage:\n",
    "# Assume param_values is already generated and has mixed columns\n",
    "param_values = enforce_one_hot_last_n_columns(param_values, n=1)\n",
    "\n",
    "# Verify the enforcement (each row should have exactly one '1' in the last 8 columns)\n",
    "print(param_values[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNpYZCxmeuEb"
   },
   "outputs": [],
   "source": [
    "param_values = torch.from_numpy(param_values).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6-CzdxighNU",
    "outputId": "19fd7b3c-5783-464c-cf4b-1f6e54427b6d"
   },
   "outputs": [],
   "source": [
    "param_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lCuu8QILam1"
   },
   "source": [
    "## Mean Variance Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NhPYZzmJazY6",
    "outputId": "48974ef5-6a85-45c2-fea7-5451c17a1c93"
   },
   "outputs": [],
   "source": [
    "network = MLP(n_inputs=13, n_hidden=[50, 50, 50], n_outputs=2, activation_fn=nn.Tanh())\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIvjwpThbjE1"
   },
   "outputs": [],
   "source": [
    "mve_model = MVERegression(\n",
    "    model=network, optimizer=partial(torch.optim.Adam, lr=1e-3), burnin_epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_F2LmqEQbSvF"
   },
   "outputs": [],
   "source": [
    "my_temp_dir = '/content/mve'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZvcbyKYbE0s",
    "outputId": "711f2736-6e49-4577-8326-147813c5b88d"
   },
   "outputs": [],
   "source": [
    "logger = CSVLogger(my_temp_dir)\n",
    "trainer = Trainer(\n",
    "    max_epochs=250,  # number of epochs we want to train\n",
    "    accelerator=\"cpu\",  # use distributed training\n",
    "    logger=logger,  # log training metrics for later evaluation\n",
    "    log_every_n_steps=1,\n",
    "    enable_checkpointing=False,\n",
    "    enable_progress_bar=False,\n",
    "    default_root_dir=my_temp_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dHSBHPUebZZJ",
    "outputId": "7a477cda-38a1-4e1b-91b7-b8b0c025bdf4"
   },
   "outputs": [],
   "source": [
    "trainer.fit(mve_model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "4zz5fV7TVqJy",
    "outputId": "1192f721-d00f-4ce2-f6ec-4a2a0d4052a2"
   },
   "outputs": [],
   "source": [
    "fig = plot_training_metrics(\n",
    "    os.path.join(my_temp_dir, \"lightning_logs\"), [\"train_loss\", \"trainRMSE\",\"trainR2\",\"trainMAE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "78PM_lUFcX2p",
    "outputId": "034171a4-349d-4078-d868-47fd9f3b6ad1"
   },
   "outputs": [],
   "source": [
    "fig = plot_training_metrics(\n",
    "    os.path.join(my_temp_dir, \"lightning_logs\"), [\"val_loss\", \"valRMSE\",\"valR2\",\"valMAE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "id": "q4s7SfFhcnCz",
    "outputId": "429c9c1b-c83b-412e-fb2f-98260cdb08fb"
   },
   "outputs": [],
   "source": [
    "preds = mve_model.predict_step(X_gtext)\n",
    "\n",
    "fig = plot_predictions_regression(\n",
    "    X_train[:,3],\n",
    "    Y_train,\n",
    "    X_gtext[:,3],\n",
    "    Y_gtext,\n",
    "    preds[\"pred\"],\n",
    "    preds[\"pred_uct\"].squeeze(-1),\n",
    "    aleatoric=preds[\"aleatoric_uct\"],\n",
    "    title=\"Mean Variance Estimation Network\",\n",
    "    show_bands=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "d8f0SiBYdOJH",
    "outputId": "26b0ecd7-f92d-448c-844b-de1eb5fb47c5"
   },
   "outputs": [],
   "source": [
    "preds = mve_model.predict_step(X_test)\n",
    "\n",
    "fig = plot_calibration_uq_toolbox(\n",
    "    preds[\"pred\"].cpu().numpy(),\n",
    "    preds[\"pred_uct\"].cpu().numpy(),\n",
    "    Y_test.cpu().numpy(),\n",
    "    X_test[:,3].cpu().numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1XUcrfALjiK"
   },
   "source": [
    "## Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7L2vGM2OyiKx"
   },
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "d57b1211908549bab805951d0fd7a81c",
      "84a60c37ce1c4acd9c589d5e5fe5bac2",
      "97909d0fee774c20b362675b535e7c48",
      "897e58e64d584cc3ad87ce3bfdd894cf",
      "55dbaf86ded1458894569d1c6cdf1b2f",
      "2331541241ac43f88bf283878bf26462",
      "d2fb7efcbe20415e865e638e068bbb0f",
      "93f999ee57604f94a3c975372d7fccc6",
      "3ac7a20000f247a092f6a27d8756c459",
      "75f9e73bd439468d892526b700b5e95f",
      "23f7f219f3e742fc8aa2a3aa5502dfca"
     ]
    },
    "id": "X3PL3VHPyjNX",
    "outputId": "f0218398-babe-4b9d-9e95-4e64b2ee5760"
   },
   "outputs": [],
   "source": [
    "# initialize the likelihood\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "# init the GP model\n",
    "gp_model = ExactGPModel(X_train.squeeze(), Y_train.squeeze(), likelihood)\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "gp_model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    gp_model.parameters(), lr=1e-2\n",
    ")  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n",
    "\n",
    "bar = tqdm(range(100))\n",
    "for i in bar:\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = gp_model(X_train.squeeze())\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, Y_train.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    bar.set_postfix(loss=f\"{loss.detach().cpu().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ya5kw4OIy03Z"
   },
   "outputs": [],
   "source": [
    "gp_model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    gp_preds = gp_model(X_gtext.cpu())\n",
    "\n",
    "gp_mean = gp_preds.mean.detach().cpu().numpy()\n",
    "gp_var = gp_preds.variance.detach().cpu().numpy()\n",
    "gp_covar = gp_preds.covariance_matrix.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "id": "P7uODTZpy8rk",
    "outputId": "898b4110-4784-4b01-97d9-05046086de85"
   },
   "outputs": [],
   "source": [
    "fig = plot_predictions_regression(\n",
    "    X_train[:,3],\n",
    "    Y_train,\n",
    "    X_gtext[:,3],\n",
    "    Y_gtext,\n",
    "    gp_mean[:, None],\n",
    "    np.sqrt(gp_var),\n",
    "    epistemic=np.sqrt(gp_var),\n",
    "    title=\"Gaussian Process\",\n",
    "    show_bands=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "wa13A0LIMRi0",
    "outputId": "a01ed1d0-4060-44a2-cfa8-7442177e39a1"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    gp_preds = gp_model(X_test.cpu())\n",
    "\n",
    "gp_mean = gp_preds.mean.detach().cpu().numpy()\n",
    "gp_var = gp_preds.variance.detach().cpu().numpy()\n",
    "gp_covar = gp_preds.covariance_matrix.detach().cpu().numpy()\n",
    "\n",
    "fig = plot_calibration_uq_toolbox(\n",
    "    gp_mean, np.sqrt(gp_var), Y_test.cpu().numpy(), X_test[:,3].cpu().numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cx-njdQiLvQd"
   },
   "source": [
    "## Laplace Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJXQ3dZGJBt6",
    "outputId": "3d80f3ee-72ed-4854-807e-839d63a76cde"
   },
   "outputs": [],
   "source": [
    "network = MLP(n_inputs=13, n_hidden=[50, 50], n_outputs=1, activation_fn=nn.Tanh())\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFj8hbCQHL3q"
   },
   "outputs": [],
   "source": [
    "deterministic_model = DeterministicRegression(\n",
    "    model=network,\n",
    "    optimizer=partial(torch.optim.Adam, lr=1e-2),\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWS4DrLqWeRV"
   },
   "outputs": [],
   "source": [
    "my_temp_dir = '/content/laplace'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbxslARhHZfs",
    "outputId": "c8ed6756-e4a3-43b5-8c79-9ac43c712abe"
   },
   "outputs": [],
   "source": [
    "logger = CSVLogger(my_temp_dir)\n",
    "trainer = Trainer(\n",
    "    max_epochs=100,  # number of epochs we want to train\n",
    "    logger=logger,  # log training metrics for later evaluation\n",
    "    log_every_n_steps=1,\n",
    "    enable_checkpointing=False,\n",
    "    enable_progress_bar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wZVCardEHzm8",
    "outputId": "3b5e8505-bfca-4171-8959-1a424b46c598"
   },
   "outputs": [],
   "source": [
    "trainer.fit(deterministic_model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "nlTihK0iH3Ts",
    "outputId": "ee5c5c4c-bcab-4d0b-9f2b-da12cad0ee0c"
   },
   "outputs": [],
   "source": [
    "fig = plot_training_metrics(\n",
    "    os.path.join(my_temp_dir, \"lightning_logs\"), [\"train_loss\", \"trainRMSE\",\"trainR2\",\"trainMAE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "Q-LLwtQvW0BZ",
    "outputId": "cb97fec9-2b82-42e0-e8c4-cca6e069ac2a"
   },
   "outputs": [],
   "source": [
    "fig = plot_training_metrics(\n",
    "    os.path.join(my_temp_dir, \"lightning_logs\"), [\"val_loss\", \"valRMSE\",\"valR2\",\"valMAE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1tyKyS7IPRP",
    "outputId": "a23410b8-bb97-49db-969a-2f0fea658920"
   },
   "outputs": [],
   "source": [
    "la = Laplace(\n",
    "    deterministic_model.model,\n",
    "    \"regression\",\n",
    "    subset_of_weights=\"last_layer\",\n",
    "    hessian_structure=\"full\",\n",
    "    sigma_noise=0.4,\n",
    ")\n",
    "\n",
    "\n",
    "laplace_model = LaplaceRegression(laplace_model=la, tune_prior_precision=True)\n",
    "\n",
    "trainer = Trainer(default_root_dir=my_temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "320a500ac0944154a4c996d536d35c24",
      "748dabb13fea4cd2a5e99bfeab171007",
      "bf581cd44a52437b91651230277bcce4",
      "6fe4d15ec1d643fb984b1342709837b0",
      "257c48068ac64e09a0d117a818c1440c",
      "028dd8f0c3fe430a9b0c46a5f4f28eaf",
      "1d7e1c870b7b416686c63ee305b168eb",
      "cbb431f211054393ac7664e8d58d7c3f",
      "f987efe4f9a84380a1327dd86a9f13ac",
      "443eb62c72b145b0b5453368a25012ef",
      "86ca91698a084756af662ad1cb920491"
     ]
    },
    "id": "teQg78DcIi4x",
    "outputId": "a554e6e4-6849-4bed-9d67-4d5b65aad14f"
   },
   "outputs": [],
   "source": [
    "trainer.test(laplace_model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Bjp1OK4IrkL"
   },
   "outputs": [],
   "source": [
    "preds = laplace_model.predict_step(X_test)\n",
    "\n",
    "fig = plot_predictions_regression(\n",
    "    X_train[:,3],\n",
    "    Y_train,\n",
    "    X_test[:,3],\n",
    "    Y_test,\n",
    "    preds[\"pred\"],\n",
    "    preds[\"pred_uct\"],\n",
    "    epistemic=preds[\"epistemic_uct\"],\n",
    "    aleatoric=preds[\"aleatoric_uct\"],\n",
    "    title=\"Laplace Approximation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "MMIm5MIZKZiz",
    "outputId": "0a3b4491-50ce-446f-a0fc-46aeef60cd0f"
   },
   "outputs": [],
   "source": [
    "preds = laplace_model.predict_step(X_test)\n",
    "fig = plot_calibration_uq_toolbox(\n",
    "    preds[\"pred\"].cpu().numpy(),\n",
    "    preds[\"pred_uct\"].numpy(),\n",
    "    Y_test.cpu().numpy(),\n",
    "    X_test[:,3].cpu().numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMRn1WxIfIwh"
   },
   "outputs": [],
   "source": [
    "sample_preds = laplace_model.predict_step(param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7w1mO0B2fPhd",
    "outputId": "129f41d1-5c3b-40df-fd11-7fb4dd11a064"
   },
   "outputs": [],
   "source": [
    "Si = sobol.analyze(problem, sample_preds[\"pred\"].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obpyhaHx5KzS"
   },
   "source": [
    "### Plot Variance Based Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_o7Ku8OwhJY4"
   },
   "outputs": [],
   "source": [
    "def plot_sensitivity_indices(Si, feature_names):\n",
    "    \"\"\"Plot the first-order and total sensitivity indices.\"\"\"\n",
    "    s1 = Si['S1']\n",
    "    st = Si['ST']\n",
    "    indices = range(len(s1))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(indices, s1)\n",
    "    plt.xticks(indices, feature_names, rotation=90)\n",
    "    plt.title('First-order Sensitivity Indices')\n",
    "    plt.ylabel('S1')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(indices, st)\n",
    "    plt.xticks(indices, feature_names, rotation=90)\n",
    "    plt.title('Total Sensitivity Indices')\n",
    "    plt.ylabel('ST')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBnv30a43gKx"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def plot_feature_variance_and_interactions(X, y, Si, feature_names, sample_fraction=0.5):\n",
    "    \"\"\"\n",
    "    Plot the variance of each feature with respect to the probability score\n",
    "    and the strongest interactions, randomly hiding scatter dots for clarity.\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): Feature matrix.\n",
    "        y (numpy.ndarray): Target values.\n",
    "        Si (dict): Sensitivity indices.\n",
    "        feature_names (list): List of feature names.\n",
    "        sample_fraction (float): Fraction of points to display in scatter plot (default is 50%).\n",
    "    \"\"\"\n",
    "    s1 = Si['S1']\n",
    "    st = Si['ST']\n",
    "    s2 = Si['S2']\n",
    "\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "        # Randomly sample a fraction of points for the scatter plot\n",
    "        total_samples = X.shape[0]\n",
    "        sample_size = int(sample_fraction * total_samples)\n",
    "        sample_indices = random.sample(range(total_samples), sample_size)\n",
    "        sampled_X = X[sample_indices, i]\n",
    "        sampled_y = y[sample_indices]\n",
    "\n",
    "        # Plot first-order sensitivity index\n",
    "        sns.scatterplot(x=sampled_X, y=sampled_y, ax=ax[0])\n",
    "        ax[0].set_title(f'Variance Contribution of {feature}', fontsize=14)\n",
    "        ax[0].set_xlabel(feature, fontsize=12)\n",
    "        ax[0].set_ylabel('lg_price', fontsize=12)\n",
    "        ax[0].tick_params(axis='x', rotation=45)\n",
    "        ax[0].tick_params(axis='y', labelsize=10)\n",
    "\n",
    "        # Calculate interaction strengths for the current feature\n",
    "        interaction_strengths = []\n",
    "        for j, other_feature in enumerate(feature_names):\n",
    "            if i != j:\n",
    "                interaction_strength = s2[i, j] if s2.ndim > 1 else 0\n",
    "                interaction_strengths.append((other_feature, interaction_strength))\n",
    "\n",
    "        # Sort by interaction strength and select top 5 for the current feature\n",
    "        interaction_strengths.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        top_interactions = interaction_strengths\n",
    "\n",
    "        interaction_features = [item[0] for item in top_interactions]\n",
    "        interaction_values = [item[1] for item in top_interactions]\n",
    "\n",
    "        # Use a colormap to assign colors\n",
    "        cmap = get_cmap('coolwarm_r')  # '_r' inverts the colormap\n",
    "        colors = [cmap(i / len(interaction_features)) for i in range(len(interaction_features))]\n",
    "\n",
    "        sns.barplot(x=interaction_features, y=interaction_values, ax=ax[1], palette=colors)\n",
    "        ax[1].set_title(f'Strongest Interactions with {feature}', fontsize=14)\n",
    "        ax[1].set_xlabel('Feature', fontsize=12)\n",
    "        ax[1].set_ylabel('Interaction Strength', fontsize=12)\n",
    "        ax[1].tick_params(axis='x', rotation=45, labelsize=10)\n",
    "        ax[1].tick_params(axis='y', labelsize=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgUNqdr-5T3E"
   },
   "source": [
    "### Continue Analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "id": "HfjLqJ-EhQdb",
    "outputId": "2bb3c606-478b-4928-c93a-21e966d041c3"
   },
   "outputs": [],
   "source": [
    "plot_sensitivity_indices(Si, all_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRjUpPVw34nh"
   },
   "outputs": [],
   "source": [
    "plot_feature_variance_and_interactions(param_values, sample_preds[\"pred\"].cpu().numpy(), Si, all_feature_names, sample_fraction=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSw2jKguPTi1"
   },
   "source": [
    "## Bayes By Backprop - Mean Field Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTuKsFe-Nk4d",
    "outputId": "08e3f235-b39d-44db-eced-1afb4ba3a9e4"
   },
   "outputs": [],
   "source": [
    "network = MLP(n_inputs=13, n_hidden=[50, 50], n_outputs=2, activation_fn=nn.ReLU())\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vibtehhOPana"
   },
   "outputs": [],
   "source": [
    "bbp_model = BNN_VI_ELBO_Regression(\n",
    "    network,\n",
    "    optimizer=partial(torch.optim.Adam, lr=3e-3),\n",
    "    criterion=NLL(),\n",
    "    stochastic_module_names=[-1],\n",
    "    num_mc_samples_train=10,\n",
    "    num_mc_samples_test=25,\n",
    "    burnin_epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "og12hbmPXKxn"
   },
   "outputs": [],
   "source": [
    "my_temp_dir = '/content/bbp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjCa3BpsPdpQ",
    "outputId": "1dbffaae-3664-4c47-f401-19e3bda7bbeb"
   },
   "outputs": [],
   "source": [
    "logger = CSVLogger(my_temp_dir)\n",
    "trainer = Trainer(\n",
    "    max_epochs=150,  # number of epochs we want to train\n",
    "    logger=logger,  # log training metrics for later evaluation\n",
    "    log_every_n_steps=20,\n",
    "    enable_checkpointing=False,\n",
    "    enable_progress_bar=False,\n",
    "    default_root_dir=my_temp_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qiDFZaYIPhKk",
    "outputId": "3ef1d5c7-4361-4715-984b-3a8d15999b40"
   },
   "outputs": [],
   "source": [
    "trainer.fit(bbp_model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "ox96gpgbPkRp",
    "outputId": "22770692-43ea-467a-921d-a1057667d5df"
   },
   "outputs": [],
   "source": [
    "fig = plot_training_metrics(\n",
    "    os.path.join(my_temp_dir, \"lightning_logs\"), [\"train_loss\", \"trainRMSE\",\"trainR2\",\"trainMAE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "b3KIRR9RXPzc",
    "outputId": "99a144fe-786d-474c-83ba-2cd75e2c589b"
   },
   "outputs": [],
   "source": [
    "fig = plot_training_metrics(\n",
    "    os.path.join(my_temp_dir, \"lightning_logs\"), [\"val_loss\", \"valRMSE\",\"valR2\",\"valMAE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "id": "FoZYR6JZPqtg",
    "outputId": "9e437097-b57c-406f-8064-6305985a13a8"
   },
   "outputs": [],
   "source": [
    "preds = bbp_model.predict_step(X_gtext)\n",
    "\n",
    "fig = plot_predictions_regression(\n",
    "    X_train[:,3],\n",
    "    Y_train,\n",
    "    X_gtext[:,3],\n",
    "    Y_gtext,\n",
    "    preds[\"pred\"].squeeze(-1),\n",
    "    preds[\"pred_uct\"],\n",
    "    epistemic=preds[\"epistemic_uct\"],\n",
    "    aleatoric=preds[\"aleatoric_uct\"],\n",
    "    title=\"Bayes By Backprop MFVI\",\n",
    "    show_bands=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "YXCVx8uYP0bU",
    "outputId": "9232a966-6fd9-47db-b40a-49f9648e3f5e"
   },
   "outputs": [],
   "source": [
    "preds = bbp_model.predict_step(X_test)\n",
    "fig = plot_calibration_uq_toolbox(\n",
    "    preds[\"pred\"].cpu().numpy(),\n",
    "    preds[\"pred_uct\"].cpu().numpy(),\n",
    "    Y_test.cpu().numpy(),\n",
    "    X_test[:,3].cpu().numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmf1oY1eX7Uy"
   },
   "source": [
    "## Bayesian Neural Network with Variational Inference and Energy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6OKcngs5QDau",
    "outputId": "7d674390-dbd0-4d2c-d712-be55653017ab"
   },
   "outputs": [],
   "source": [
    "network = MLP(n_inputs=8, n_hidden=[50, 50], n_outputs=1, activation_fn=nn.Tanh())\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F03mumLmYMF4"
   },
   "outputs": [],
   "source": [
    "bnn_vi_model = BNN_VI_Regression(\n",
    "    network,\n",
    "    optimizer=partial(torch.optim.Adam, lr=1e-2),\n",
    "    n_mc_samples_train=10,\n",
    "    n_mc_samples_test=50,\n",
    "    output_noise_scale=1.3,\n",
    "    prior_mu=0.0,\n",
    "    prior_sigma=1.0,\n",
    "    posterior_mu_init=0.0,\n",
    "    posterior_rho_init=-6.0,\n",
    "    alpha=1e-03,\n",
    "    bayesian_layer_type=\"reparameterization\",\n",
    "    stochastic_module_names=[-1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzuAUJt6YUjI"
   },
   "outputs": [],
   "source": [
    "my_temp_dir = '/content/bnn_vi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UCY2cSKPYQ6S",
    "outputId": "e70765fc-f5df-4a54-c64f-960b4fac26c6"
   },
   "outputs": [],
   "source": [
    "logger = CSVLogger(my_temp_dir)\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='valRMSE',  # You can change this to another metric like 'val_accuracy'\n",
    "    patience=10,  # Number of epochs with no improvement before stopping\n",
    "    verbose=False,  # Print when early stopping is triggered\n",
    "    mode='min',  # 'min' means we are looking for a decrease in the metric (e.g., val_loss)\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=200,  # number of epochs we want to train\n",
    "    logger=logger,  # log training metrics for later evaluation\n",
    "    log_every_n_steps=1,\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=False,\n",
    "    limit_val_batches=1.0,  # full validation runs\n",
    "    default_root_dir=my_temp_dir,\n",
    "    callbacks=[early_stop_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xQOeU_y4Yake",
    "outputId": "76aca115-e88b-410c-ad13-c251125bd7de"
   },
   "outputs": [],
   "source": [
    "trainer.fit(bnn_vi_model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "hhVmBkKHYe2I",
    "outputId": "7e14a7a4-68d9-4af7-fef3-03819aa174b0"
   },
   "outputs": [],
   "source": [
    "fig = plot_training_metrics(\n",
    "    os.path.join(my_temp_dir, \"lightning_logs\"), [\"train_loss\", \"trainRMSE\",\"trainR2\",\"trainMAE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "s34ZQmwRYpv9",
    "outputId": "7df28c72-b92a-4da7-c199-890884f6e339"
   },
   "outputs": [],
   "source": [
    "fig = plot_training_metrics(\n",
    "    os.path.join(my_temp_dir, \"lightning_logs\"), [\"val_loss\", \"valRMSE\",\"valR2\",\"valMAE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "id": "rOcHz4S2a-eF",
    "outputId": "0b45c1f0-72bf-403f-a21e-50697f35ed72"
   },
   "outputs": [],
   "source": [
    "preds = bnn_vi_model.predict_step(X_test)\n",
    "\n",
    "fig = plot_predictions_regression(\n",
    "    X_train[:,3],\n",
    "    Y_train,\n",
    "    X_test[:,3],\n",
    "    Y_test,\n",
    "    preds[\"pred\"],\n",
    "    preds[\"pred_uct\"],\n",
    "    epistemic=preds[\"epistemic_uct\"],\n",
    "    show_bands=False,\n",
    "    title=\"Bayesian NN with Alpha Divergence Loss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "q16-mZOhbEje",
    "outputId": "d86fb560-32c5-43da-9aeb-8fc8093f47e0"
   },
   "outputs": [],
   "source": [
    "preds = bnn_vi_model.predict_step(X_test)\n",
    "fig = plot_calibration_uq_toolbox(\n",
    "    preds[\"pred\"].cpu().numpy(),\n",
    "    preds[\"pred_uct\"].cpu().numpy(),\n",
    "    Y_test.cpu().numpy(),\n",
    "    X_test[:,3].cpu().numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yrcn557Cg_u"
   },
   "outputs": [],
   "source": [
    "pred_mean = preds[\"pred\"].cpu().numpy().squeeze()\n",
    "pred_std = preds[\"pred_uct\"].cpu().numpy()\n",
    "te_y = np.squeeze(Y_test.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "Vwlh6IQM_Hpn",
    "outputId": "ca474b7f-7196-4af8-ef12-6377d8dabfe6"
   },
   "outputs": [],
   "source": [
    "# Plot adversarial group calibration\n",
    "uct.viz.plot_adversarial_group_calibration(preds[\"pred\"].cpu().numpy().squeeze(), preds[\"pred_uct\"].cpu().numpy(), np.squeeze(Y_test.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aqy_BB2rAURA",
    "outputId": "70439343-91a6-4b87-c990-d5aaa432ebe2"
   },
   "outputs": [],
   "source": [
    "uct.metrics.get_all_metrics(preds[\"pred\"].cpu().numpy().squeeze(), preds[\"pred_uct\"].cpu().numpy(), np.squeeze(Y_test.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HghLUzpYEPK6"
   },
   "outputs": [],
   "source": [
    "test_df['pred_mean'] = pred_mean\n",
    "test_df['pred_std'] = pred_std\n",
    "test_df['residual'] = pred_mean - te_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O61ajyEiEay8"
   },
   "outputs": [],
   "source": [
    "test_df.to_csv('test_df_flood_with_loc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dT6klJNVjX86"
   },
   "outputs": [],
   "source": [
    "sample_preds = bnn_vi_model.predict_step(param_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u8Qce54Vje4L",
    "outputId": "14741eab-e475-465d-8db5-38ff23a330fc"
   },
   "outputs": [],
   "source": [
    "Si = sobol.analyze(problem, sample_preds[\"pred\"].cpu().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "id": "qzb1DD1YjpjQ",
    "outputId": "8dee367b-2626-4e37-d505-3fd3f840476a"
   },
   "outputs": [],
   "source": [
    "plot_sensitivity_indices(Si, all_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pI_02OxL-doa"
   },
   "outputs": [],
   "source": [
    "plot_feature_variance_and_interactions(param_values, sample_preds[\"pred\"].cpu().numpy().squeeze(), Si, all_feature_names, sample_fraction=0.1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3lCuu8QILam1",
    "o1XUcrfALjiK",
    "cSw2jKguPTi1"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
